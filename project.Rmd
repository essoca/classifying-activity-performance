---
title: 'Classifying Performance in the Weight Lifting Excercises Dataset'
author: "essoca"
date: "June 28, 2018"
output: html_document
---

### Overview
In the field of human activity recognition, it is usual to ask *how much* of a particular activity people do but not *how well* they do it. In this project, an alternative analysis of part of the work of E. Velloso *et al* on [Qualitative Activity Recognition of Weight Lifting Exercises](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201) is made as per the requirements of the Practical Machine Learning course of the Johns Hopkins Data Science Specialization. The purpose is to model the recognition of the performance of 5 different ways to do a weight-lifting excersise (one of them being the pre-designed correct way) from data taken by accelerometers on the belt, forearm, arm, and dumbell of 6 participants of an experiment.

### Exploring the training data set


```{r}
    training <- read.csv("training.csv", na.strings = c("","NA","#DIV/0!"), stringsAsFactors = FALSE)
    dim(training)
```

The 5 activities are coded in the variable "classe" as characters A, B, C, D, and E.
```{r}
    training <- transform(training, classe = as.factor(classe))
```
In the experiment, there are four sensors: Belt, Glove, Arm-band, and Dumbbell. These provide three-axes acceleration, gyroscope, and magnetometer readings at a joint sampling rate of 45 Hz (45 samples each second). Euler angles measuring orientation are called: roll, pitch, and yaw.

The raw data for the 3 directions of magnetometer, gyroscope and acceleration has 4x3x3 = `r 4*3*3` features. There are, however, 8 derived variables related to acceleration

```{r}
    length(names(training)[grep("(var|total).*accel", names(training))])
```

thus giving 44 features:

```{r}
    length(grep("magnet|gyros|accel", names(training)))
```

For each Euler angle (roll, pitch and yaw) measured by each of the four sensors, the following 8 derived features are extracted by the authors: mean, variance, standard deviation, max, min, amplitude, kurtosis, and skewness. These give 4x3 (raw) + 4x3x8 (derived) = `r 4*3+4*3*8` features related to Euler angles 
```{r}
    length(grep("roll|pi(tch|cth)|yaw", names(training)))
```
giving a total of 108+44=`r 108+44` features.

The remaining 8 of the total 160 columns of the training set are
```{r}
    numer_features <- grep("magnet|gyros|accel|roll|pi(tch|cth)|yaw", names(training))
    names(training)[-numer_features]
```
Here "X" is just numbering the rows and the window variables refer to the time window for feature extraction considered by the authors.
 
#### A large number of missing values

The percent of missing values in the training data set is
```{r}
    round(sum(is.na(training))/prod(dim(training)),2)*100
```

So we begin disregarding features with no values whatsoever and input values according to the mean
```{r}
    features <- training[, numer_features]
    # Remove features with missing values for all cases
    allNa <- which(sapply(features, function(x) all(is.na(x)))); names(allNa) <- NULL
    features <- features[, -allNa]
    # Replace NA values by average of feature
    features[] <- lapply(features, function(x) suppressWarnings(as.numeric(x)))
    replaceNa <- function(feature) ifelse(is.na(feature), mean(feature, na.rm = TRUE), feature)
    features[] <- lapply(features, replaceNa)
    # If a feature does not show variation after this, remove it as well
    allSame <- which(sapply(features, function(x) sd(x)) == 0); names(allSame) <- NULL
    features <- features[, -allSame]
```


### Model selection

There are a bunch of features which do not seem, from physical intution, to contribute to the recognition of the specified task. So we first remove those features that are highly correlated (above a 85% level).
```{r}
    suppressMessages(library(caret))
     # Evaluate the correlation matrix
    corrMatrix <- cor(features)
    highCorr <- findCorrelation(corrMatrix, cutoff = 0.85) 
    features <- features[, -highCorr]
    features$classe = training$classe
```

#### Cross-validation

In order to have an idea of the out-of-sample error, we do cross-validation by taking each of 10 folds of samples and training a random forest in a randomly selected fold from the remaining ones. The average accuracy of the best model selected from the forest is then shown.
```{r, eval = TRUE}
    # Create folds for cross-validation
    set.seed(1212)
    folds <- createFolds(features$classe, k = 10, list = TRUE)
    foldTrainIndices <- 1:10
    foldTrainBest <- NULL
    accuracyK <- NULL
    for(k in foldTrainIndices){
        foldTrainIndex <- sample(foldTrainIndices[-k], 1)
        foldTrainBest <- c(foldTrainBest, foldTrainIndex)
        model <- train(classe ~ ., method = "rf", data = features[folds[[foldTrainIndex]], ])
        cMK <- confusionMatrix(predict(model, features[folds[[k]], ]), features[folds[[k]], ]$classe)
        accuracyK <- c(accuracyK, cMK$overall[1])
        print(paste("Accuracy in fold k:", round(cMK$overall[1],3)))
    }
    print(paste("Average accuracy:", round(mean(accuracyK),3)))
    # Select fold with best accuracy for final model
    bestFold <- foldTrainBest[which.max(accuracyK)]
    model <- train(classe ~ ., method = "rf", data = features[folds[[bestFold]], ])
    save(model, file = "model.rda")
```

### Classifying activity performance

We now make the predictions on the test set. First we load and explore the data

```{r eval = TRUE}
    testing <- read.csv("testing.csv", na.strings = c("","NA","#DIV/0!"), stringsAsFactors = FALSE)
    #testing <- transform(testing, classe = as.factor(classe))
    dim(testing)
    testfeatures <- testing[, numer_features]
    # Drop features not considered in training
    testfeatures <- testfeatures[, -c(allNa, allSame)]; testfeatures <- testfeatures[, -highCorr]
    # There are extra features (different from the training set) with all cases missing
    allNaTesting <- which(sapply(testfeatures, function(x) all(is.na(x)))); names(allNaTesting) <- NULL
    # Randomly input values in corresponding positions from the training set
    testfeatures[, allNaTesting] <- features[sample(1:nrow(features), nrow(testing)), allNaTesting]
    # Also some features used to train the model do not appear in the test set
    missing_features <- setdiff(names(features), names(testfeatures)); missing_features
    # Input the values from the training set
    testfeatures$min_pitch_dumbbell <- features$min_pitch_dumbbell[sample(1:nrow(features), nrow(testing))]
    testfeatures$min_roll_forearm <- features$min_roll_forearm[sample(1:nrow(features), nrow(testing))]
    # Replace NA values by average of feature
    testfeatures[] <- lapply(testfeatures, function(x) suppressWarnings(as.numeric(x)))
    testfeatures[] <- lapply(testfeatures, replaceNa)
    # Apply the selected model
    load("model.rda")
    pred <- predict(model, newdata = testfeatures);
    df_prediction <- testing[, 2:5]; df_prediction$prediction <- pred; 
    names(df_prediction)[2:3] <- c('time_stamp_part1', 'time_stamp_part2'); df_prediction
```

These predictions are to be contrasted with the observations in the quiz.
